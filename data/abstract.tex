% !TeX root = ../main.tex

% 中英文摘要和关键字

\begin{abstract}
基于Transformer的大语言模型（LLM）在智能对话、代码生成以及智能体系统等领域取得了显著成功，展现出通往通用人工智能的巨大潜力。
然而，由于大模型参数规模巨大，其全生命周期对计算、存储、通信及能耗等资源提出了极高的需求，制约了其进一步地发展。若其中任一资源的供给不足成为瓶颈，不仅会限制系统整体性能，还会导致其他资源的闲置与浪费。
在推理阶段，\textbf{能效}是制约服务规模扩展的重要因素。受限于数据中心电力供应极限，需要在延迟约束下追求更高的能效比以缓解容量压力。
在训练阶段，\textbf{通信}是制约计算利用率的关键瓶颈。在互联带宽受限的集群中，传统并行方案的通信开销会随着大模型上下文长度的增长而增大，严重拖累了整体的训练吞吐，需要新的并行范式来降低通信压力。
\textbf{并行策略}设计是制约异构资源高效利用的另一大难点。合理的策略优化能够有效利用充裕资源缓解稀缺资源的瓶颈。然而，面对多样化的硬件环境，依赖人工经验的静态策略难以适配多维资源约束，无法全面探索优化空间，亟需高效的自动化策略发现机制。
基于以上三个问题，本文从能效优化、通信优化和并行策略自动发现三个维度展开研究。研究内容和贡献如下：

（1）针对大模型推理服务中的能效问题，本文揭示了LLM推理存在非单调的“U型能耗-频率”曲线以及由硬件分块机制导致的“能效阶梯”现象。
据此，本文设计并实现了GreenServe系统，通过精细化的频率调节与请求调度，
在保障整体推理服务质量前提下，实现了相比原先静态策略高达36.3\%的端到端能耗降低。

（2）针对长上下文大模型训练中中间激活值通信量爆炸的问题，本文深入分析了通信开销与模型架构的关系，提出了权重流水线并行新范式WeiPipe。
与传统的传递激活值的流水线并行不同，WeiPipe通过在设备间流转模型权重及其梯度，将通信量从与序列长度和模型大小相关解耦为仅与模型大小相关。
实验表明，在长序列训练场景下，WeiPipe相比现有主流的流水线方案实现了30\%至80\%的吞吐量提升。

（3）针对并行策略设计难题，本文提出了基于数据一元论的自动发现框架CATFlow。将并行计划建模为约束感知张量的时空轨迹优化问题，通过自动搜索发现了包括ZeRO-3.5在内的新型高效策略，不引入额外开销的前提下，能够在不同配置下相比经典策略提升达39.1\%的吞吐。


% 动态资源分配 -> 根据负载动态调整资源分配，节省资源浪费
% 高熵资源利用 -> 非瓶颈的情况下尽可能多的去使用资源
% 异构资源调度 -> 使用更多的非受限资源来节省受限资源


  % 关键词用“英文逗号”分隔，输出时会自动处理为正确的分隔符
  \thusetup{
    keywords = {大语言模型,分布式计算,并行优化, 能效优化},
  }
\end{abstract}

% \begin{abstract*}


%   % Use comma as separator when inputting
%   \thusetup{
%     keywords* = {keyword 1, keyword 2, keyword 3, keyword 4, keyword 5},
%   }
% \end{abstract*}
