面向大规模语言模型的高效训练与推理系统关键技术研究第一章 绪论1.1 研究背景与意义随着人工智能技术的迅猛发展，以Transformer架构为基础的大规模语言模型（Large Language Models, LLMs）已成为推动通用人工智能（AGI）发展的核心动力。从早期的GPT-2到如今的GPT-4、Llama 3、DeepSeek等模型，参数规模已从亿级跃升至万亿级，这种“缩放定律”（Scaling Laws）在带来模型性能飞跃的同时，也使得底层计算系统面临前所未有的挑战 1。当前，大模型的全生命周期——涵盖推理服务（Inference Serving）、预训练（Pre-training）及微调（Fine-tuning）——正遭遇“能耗墙”、“通信墙”与“内存墙”的三重制约。在推理阶段，大模型的部署成本与能耗已成为制约其大规模应用的主要障碍。LLM推理具有独特的预填充（Prefill）和解码（Decode）两阶段特征，且对服务等级目标（SLOs）如首字延迟（TTFT）和代币生成间隔（ITL）有严格要求。传统的单体部署或简单的批处理策略难以在保证服务质量的同时最小化能耗。特别是随着生成式AI应用的普及，数据中心的推理能耗占比急剧上升，如何在保证SLO的前提下实现绿色推理（Green AI）成为迫切需求 3。在训练阶段，随着长上下文（Long Context）应用需求的爆发，如长文档摘要、代码生成及基因序列分析，训练序列长度已从标准的4K扩展至128K甚至1M token。这种长序列训练导致中间激活值（Activation）呈线性甚至超线性增长，使得显存需求急剧膨胀。传统的并行策略在处理长上下文时，往往受限于通信带宽瓶颈或流水线气泡（Pipeline Bubble），难以维持高效的设备利用率 3。此外，现有的分布式训练策略设计高度依赖专家经验，面对异构硬件与多变模型结构的组合，缺乏能够自动发现最优并行计划（Parallelism Plan）的机制 3。针对上述挑战，本博士学位论文围绕“系统高效性”这一核心目标，按照“从服务侧能效优化到训练侧通信优化，再到通用策略自动搜索”的逻辑主线展开深入研究。本文旨在通过系统级的创新，打破物理资源的限制，提升硬件资源利用率，并为大模型的可持续发展提供理论支撑与技术解决方案。1.2 本文主要研究内容与贡献针对大模型系统面临的上述挑战，本文提出了三项核心技术工作：能效与SLO感知的推理服务系统（GreenServe）：针对推理服务中日益严峻的能耗问题，在PD分离（Prefill-Decode Disaggregation）架构下，揭示了LLM推理中U型能耗-频率曲线及架构粒度导致的能效阶梯现象。设计了EcoFreq（频率控制器）、EcoPred（延迟预测器）和EcoRoute（状态空间路由器），实现了在保障SLO前提下的显著节能 3。面向长上下文训练的权重流水线并行（WeiPipe）：针对长上下文训练场景下激活值通信量爆炸的问题，提出了“权重流水线”（Weight Pipeline）的新范式。通过在流水线阶段间传递模型权重而非激活值，并设计WeiPipe-Interleave与WeiPipe-zero-bubble等调度策略，显著降低了通信开销并减少了流水线气泡 3。数据一元论视角的并行策略自动发现（CATFlow）：针对现有自动并行框架表达能力不足的问题，提出了“数据一元论”（Data-Monistic）抽象。通过约束感知张量（Constraint-Aware Tensor, CAT）和显式的数据流原语，构建了完备的并行策略搜索空间，并利用势能下降算法自动发现了包括ZeRO-3.5、ZeRO-4在内的新型高效并行策略 3。1.3 论文组织结构本文共分为六章，各章安排如下：第一章：绪论。介绍研究背景、意义、主要研究内容及论文结构。第二章：相关工作。综述大模型推理系统、训练并行策略及自动并行化技术的研究现状。第三章：介绍基于自适应频率控制与状态空间路由的节能推理系统（GreenServe）。重点解决推理阶段的能效与SLO保障问题。第四章：介绍面向长上下文训练的通信高效权重流水线并行方法（WeiPipe）。重点解决长序列训练中的通信瓶颈问题。第五章：介绍基于数据一元论抽象的分布式训练策略自动发现框架（CATFlow）。重点解决复杂并行策略的自动搜索与优化问题。第六章：总结与展望。总结全文工作，并对未来研究方向进行展望。第二章 相关工作2.1 大模型推理服务系统推理系统的优化主要集中在提高吞吐量和降低延迟。vLLM提出的PagedAttention通过非连续内存管理解决了KV Cache的显存碎片问题；Orca引入了连续批处理（Continuous Batching）以提升动态负载下的吞吐量。近年来，分离式架构（Disaggregated Architecture）即PD分离（Prefill-Decode Disaggregation）成为热点，DistServe、SplitWise等工作证明了将计算密集型的Prefill阶段与访存密集型的Decode阶段分离部署可以显著提升性能 4。然而，现有工作多关注性能指标，鲜有研究针对PD分离架构下的能效优化，尤其是如何在满足SLO的前提下利用动态频率调节（DVFS）和请求路由来降低能耗 3。2.2 大模型分布式训练并行策略分布式训练是解决大模型显存与算力缺口的必经之路。目前主流的并行策略主要分为以下几类：数据并行（DP）及其变体：如ZeRO系列技术（ZeRO-1/2/3），通过切分优化器状态、梯度和参数降低显存需求，但引入了大量通信开销 1。模型并行（MP）：包括张量并行（TP）和流水线并行（PP）。PP通过将模型层切分到不同设备并利用微批次掩盖计算空闲。然而，在长上下文训练场景下，PP面临着巨大的中间激活值传输压力。传统的PP策略以传递激活值（Activation-passing）为主，其通信量与序列长度成正比，在长序列场景下成为主要瓶颈 5。2.3 并行策略搜索与自动并行化为了应对复杂的并行空间，学术界提出了多种自动并行化框架。Alpa、FlexFlow、Unity等系统尝试在给定的算子图上搜索最优的切分策略。然而，现有的搜索方法多基于“以算子为中心”（Operator-centric）的抽象，将数据移动视为算子调度的副作用。这种抽象缺乏对数据生命周期和细粒度通信轨迹的显式建模，限制了搜索空间。许多涉及复杂数据流协同优化（如ZeRO-Offload的变体、复杂的重计算策略）的新型策略无法被这些框架自动发现 3。第三章 基于自适应频率控制与状态空间路由的节能推理系统3.1 引言随着大模型应用从云端向边缘延伸，以及推理服务规模的指数级增长，GPU集群的能耗成本日益高昂。现有的推理系统（如vLLM, TGI）主要关注吞吐量（Throughput）和延迟（Latency），往往采用静态的最高频率运行GPU，导致了非必要的能耗浪费。此外，LLM推理具有显著的Prefill（计算密集）和Decode（访存密集）阶段差异，且Decode阶段的Batch Size变化会导致算力利用率的剧烈波动。单一的频率策略无法在满足严格SLO（TTFT, ITL）的同时实现能效最优。本章提出了GreenServe系统，这是首个在PD分离架构下实现细粒度能效优化的推理服务系统 3。3.2 LLM推理能耗特性分析3.2.1 U型能耗-频率曲线通过对NVIDIA A100 GPU的深入Profiling，本研究首次揭示了LLM推理中存在的非单调U型能耗-频率关系。物理模型：GPU功率 $P$ 由静态功耗和动态功耗组成。低频区：降低频率虽然降低了瞬时功率，但显著延长了推理时间。由于静态漏电功耗的存在，导致总能耗（$Energy = Power \times Time$）反而上升。高频区：提高频率虽缩短了时间，但动态功率呈超线性增长，导致总能耗同样上升。甜蜜点（Sweet Spot）：因此，存在一个中间频率（如A100上的1005 MHz），在此频率下能耗达到最低值。重要的是，Prefill阶段（计算密集）和Decode阶段（访存密集）的甜蜜点频率是不同的，这为分阶段优化提供了理论依据 3。3.2.2 架构粒度导致的能效阶梯研究还发现了一个关键的架构现象：能效阶梯（Staircase Effect）。当Decode阶段的Batch Size跨越特定阈值（如256，这与GPU Thread Block的调度粒度相关）时，会触发GPU内核的Tiling机制变化。这导致计算单元的利用率发生突变，形成延迟和能耗的“阶梯状”跃升。3.3 GreenServe系统设计GreenServe基于PD分离（Prefill-Decode Disaggregation）架构，引入了三个核心组件来利用上述特性，实现SLO感知的能效优化。3.3.1 EcoPred：轻量级在线延迟预测器为了在毫秒级的时间尺度上进行精准控制，系统必须准确预测不同频率和负载下的推理延迟。设计：EcoPred基于XGBoost构建轻量级预测模型。在线自适应：由于实际请求长度分布可能与离线Profiling数据存在偏差，EcoPred利用在线收集的运行数据进行实时微调，确保持续的高预测精度 3。3.3.2 EcoFreq：迭代级响应式频率控制器针对工作负载的瞬时波动，EcoFreq实现了**迭代级（Per-iteration）**的频率调节。机制：在每个推理步（Iteration）开始前，EcoFreq根据EcoPred的预测结果决策。策略：如果低频运行足以在截止时间内完成计算（满足SLO），EcoFreq将频率降低至甜蜜点附近以最大化节能；如果SLO紧迫，则瞬间提升频率至最高 3。3.3.3 EcoRoute：状态空间路由器针对“能效阶梯”现象，EcoRoute设计了一种状态空间导向的请求分发策略。状态空间建模：将每个Decode实例的状态建模为（Batch Size, KV Cache占用量）二维空间中的一个点。决策逻辑：当新请求到达时，EcoRoute通过模拟（What-if Analysis）预测将请求路由到不同实例后的频率需求。它倾向于将请求路由到那些“增加负载不会触发频率大幅提升或跨越能效阶梯”的实例。这种非均匀的路由策略避免了所有实例同时运行在低效的高频高负载状态 3。3.4 实验评估与分析3.4.1 实验设置实验在包含NVIDIA A100和GH200 GPU的集群上进行，使用SGLang作为基座推理框架实现了GreenServe。测试负载采用了真实的Azure LLM推理轨迹（Azure Trace）和ShareGPT数据集。3.4.2 实验结果分析能耗节约：相比于静态最高频率运行的基线（SGLang-MaxFreq），GreenServe在保证TTFT和ITL SLO达成率持平的前提下，实现了最高**36.3%**的端到端能耗降低。泛化能力：该方法在不同的GPU架构（A100 vs GH200）和不同的模型尺寸（LLaMA-8B/70B）上均表现出一致的节能效果。GH200上的实验表明，即使在拥有更强算力的硬件上，精细化的频率控制依然能带来显著的能效收益 3。3.5 本章小结GreenServe通过软硬协同的设计，首次在PD分离架构中引入了频率与路由的联合优化。它证明了在大模型推理中，“快”并不一定意味着“全速”，通过EcoFreq和EcoRoute实现的精细化“按需计算”是实现绿色AI的关键路径。第四章 面向长上下文训练的通信高效权重流水线并行方法4.1 引言在解决了推理侧的能效问题后，我们将视线转向大模型能力的源头——训练阶段。在大模型训练领域，序列长度（Sequence Length, $S$）的扩展已成为提升模型能力的关键方向。然而，长上下文给分布式训练带来了独特的挑战。在传统的流水线并行（PP）中，前向传播需要将每一层的输出激活值传递给下一个阶段。当序列长度 $S$ 显著增加时，激活值的通信量将远远超过模型权重本身。传统的“激活值传递”（Activation-passing）模式在长上下文场景下效率急剧下降，成为系统的主要瓶颈 3。4.2 权重流水线并行（WeiPipe）机制4.2.1 核心思想与理论推导WeiPipe的核心洞察在于重新审视了通信内容的开销比例。当 $GS > 12H$ 时（$G$为微批次大小，$S$为序列长度，$H$为隐藏层维度），传输权重比传输激活值更具通信效率。对于现代大模型（如Llama-2-7B），在长序列设置下，该比率远大于1。基于此，本文提出了WeiPipe：每个Worker固定持有一部分数据的激活值，而模型权重及其梯度以流水线的方式在Worker之间循环流动。这种“权动数不动”的范式彻底改变了通信模式 3。4.2.2 WeiPipe-Naive：基础权重流转策略在逻辑环状拓扑中，权重在Worker间传递。前向传播：Worker利用传入的权重和本地输入计算激活值，保存激活值后将权重传给下一个Worker。反向传播：反向传播需要按相反顺序使用权重。WeiPipe-Naive在环中同时传输前向权重和反向权重。权重更新：权重梯度也会在环中流动并聚合。4.3 高级调度策略设计为了进一步优化性能，本文提出了两种高级调度策略：WeiPipe-Interleave 和 WeiPipe-zero-bubble。4.3.1 WeiPipe-Interleave：交错式权重流水线WeiPipe-Interleave旨在降低流水线气泡率并提高带宽利用率。其核心策略是将前向（Forward）和反向（Backward）过程交错执行。交错执行机制：Worker在等待反向所需权重的间隙，可以利用手中持有的前向权重处理新的微批次，从而填充流水线气泡。双向复用与通信减半：利用环形拓扑的对称性，特定时刻传输的数据块既能服务于当前的前向计算，也能被缓存用于稍后的反向计算，将计算/通信比提升了一倍 3。4.3.2 WeiPipe-zero-bubble：零气泡流水线探索受Zero Bubble PP启发，本文进一步探索了将反向传播拆分为“激活梯度计算”（B pass）和“权重梯度计算”（W pass）两个独立阶段在权重流水线中的应用。WZB1 & WZB2：通过解耦B pass和W pass的依赖，允许W pass被推迟或与前向计算重叠，实现了接近零气泡的流水线调度。特别是WZB2策略，在显存充足的场景下能提供极致的吞吐量 3。4.4 实验评估与分析4.4.1 实验环境与结果实验在NVIDIA A800 GPU集群上进行。结果表明：显著的吞吐量提升：在长上下文（4K-16K token）配置下，WeiPipe-Interleave相比于SOTA的流水线并行（1F1B）和FSDP，吞吐量提升了30%至80%。通信瓶颈的突破：在PCIe和以太网互联的受限带宽环境中，WeiPipe凭借恒定的权重通信量，保持了较高的计算效率，展现了极高的鲁棒性 3。4.5 本章小结本章提出的WeiPipe通过“权动数不动”的范式转换，从根本上解决了长上下文训练中的激活值通信瓶颈。这为训练更长上下文的大模型提供了高效的系统支持。第五章 基于数据一元论抽象的分布式训练策略自动发现框架5.1 引言上一章的WeiPipe通过专家设计的特定策略解决了长上下文通信问题。然而，大模型训练场景千变万化，能否有一种通用的方法自动发现像WeiPipe这样高效的策略，甚至发现人类专家未曾设想的新策略？现有的自动并行化框架多基于“以算子为中心”的抽象，难以表达涉及复杂数据流协同优化的策略。为了突破这一局限，本章提出了CATFlow框架，旨在通过全新的“数据一元论”（Data-Monistic）抽象，自动发现未知的、更高效的并行策略 3。5.2 数据一元论（Data-Monistic）抽象体系5.2.1 核心理念：数据轨迹决定计算CATFlow的核心洞察是：并行计划本质上是对数据时空轨迹的描述。计算不再是第一性的，而是被隐式定义为一种约束：当所需的输入张量在同一时空坐标汇聚时，计算自然发生。通过描述数据的轨迹，我们实际上统一了计算、存储和通信的调度。5.2.2 约束感知张量（Constraint-Aware Tensor, CAT）CATFlow引入了CAT作为系统的基本图元。一个CAT不仅代表数据块本身，还封装了其生命周期内的所有时空约束。时空坐标：包括空间（设备ID）和时间（逻辑时间戳）。约束：包括张量内约束（生命周期轨迹）和张量间约束（依赖关系）。5.3 策略空间构建与原语设计5.3.1 减法式空间构建（Subtractive Methodology）CATFlow采用“减法式”方法构建搜索空间。系统初始假设所有合法的（满足数据依赖的）数据轨迹都是允许的通用集合。用户通过原语添加约束（如domain，not），逐步裁剪掉不可行的区域。这种方法使得搜索空间天然包含了那些人类专家未曾显式定义的策略，为发现新策略提供了可能 3。5.3.2 操作原语CATFlow提供了一组操作CAT的原语，包括分布原语（split, assign, copy）、依赖管理原语（order, redirect）等，用于灵活地构建和操纵并行策略。5.4 自动发现的新型策略：ZeRO-3.5 与 ZeRO-4利用CATFlow框架及其势能下降搜索算法，本研究自动发现了两种优于现有ZeRO-3的新策略，命名为ZeRO-3.5和ZeRO-4 3。5.4.1 ZeRO-3.5：权重驻留策略CATFlow发现，在反向传播过程中，随着激活值显存的释放，可以推迟权重的释放操作。即在反向传播中保留收集好的全量权重，直接供下一个Step的前向传播使用。这消除了前向传播阶段的All-Gather通信，减少了50%的权重通信量，且不增加峰值显存占用。5.4.2 ZeRO-4：梯度延迟规约策略CATFlow进一步探索了梯度的生命周期，建议将部分梯度的Reduce-Scatter操作推迟到下一个前向传播阶段执行。这利用了前向传播通信相对空闲的特点，将反向阶段密集的通信压力分摊到前向阶段。相比ZeRO-3，ZeRO-4实现了最高39.1%的吞吐量提升，同时显存占用相比ZeRO-2降低了36.6% 3。5.5 本章小结CATFlow通过建立“数据一元论”的抽象与搜索理论，突破了人工设计并行策略的局限。自动发现的ZeRO-3.5和ZeRO-4策略不仅在性能上超越了现有方案，更验证了该框架在复杂约束下发现非直观优化策略的能力，实现了从特定优化（如WeiPipe）向通用自动优化的跨越。第六章 总结与展望6.1 全文总结本文针对大模型时代的系统效率挑战，提出了一套覆盖推理到训练的全栈优化方案。首先，GreenServe通过频率与路由的精细协同，解决了推理服务的能效与SLO保障问题；其次，WeiPipe通过权重流动的范式创新，突破了长上下文训练的通信墙；最后，CATFlow通过数据一元论的抽象与自动搜索，实现了复杂并行策略的自动发现，为系统的持续优化提供了通用方法论。6.2 创新点总结发现了LLM推理的U型能耗曲线规律并设计了能效感知系统（GreenServe）：设计了首个PD分离架构下的能效优化系统，通过迭代级频率控制和状态空间路由，实现了高性能与低能耗的统一。提出了权重流水线（WeiPipe）并行范式：在长序列训练场景下，通过传递权重替代传递激活值，重构了通信模式，显著降低了通信开销。建立了数据一元论（Data-Monistic）的并行策略描述与搜索理论（CATFlow）：提出了CAT抽象和势能下降搜索算法，实现了复杂、非直观并行策略（如ZeRO-3.5/4）的自动发现。6.3 未来展望未来工作将进一步探索将CATFlow的自动搜索思想应用于推理阶段，实现训练推理一体化的策略搜索；同时，研究上述技术在国产异构硬件上的适配与优化，推动大模型系统生态的进一步完善。攻读博士学位期间发表的学术论文与研究成果GreenServe: Energy-Efficient and SLO-Aware Disaggregated LLM Serving via Adaptive Frequency Control and State-Space Routing[Author List hidden for review]The 53rd International Symposium on Computer Architecture (ISCA 2026) (Under Review). (CCF-A类会议) 3WeiPipe: Weight Pipeline Parallelism for Communication-Effective Long-Context Large Model TrainingJunfeng Lin, et al.Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (PPoPP '25). (CCF-A类会议, 已发表) 3CATFlow: Data-Monistic Parallelism Plan Discovery for Distributed Training[Author List hidden for review]Operating Systems Design and Implementation (OSDI) / Symposium on Operating Systems Principles (SOSP) (In Preparation). 3致谢（在此处撰写致谢内容。）